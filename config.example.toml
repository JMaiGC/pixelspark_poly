bind_address = "0.0.0.0:3000"
max_concurrent = 5

# Leave out or add "*" as allowed origin to allow any
allowed_origins = [
	"https://localhost:3000"
]

allowed_keys = [
	"foo"
]

# To allow usage without any key
# public = true


[models.gpt2dutch]
model_path = "./data/gpt2-small-dutch-f16.bin"
architecture = "gpt2"
threads_per_session = 8

[tasks.gpt2dutch]
model = "gpt2dutch"

[models.pythia160m]
model_path  = "./data/pythia-160m-q4_0.bin"
architecture = "gptneox"
threads_per_session = 1
context_size = 2048

[models.mpt_chat]
model_path = "mpt-7b-chat-q5_1-ggjt.bin"
architecture = "mpt"
threads_per_session = 8

[memories.test]
embedding_model = "orcamini3b"
dimensions = 3200
store = { hora = { path = "test.index" } }
chunk_separators = ["."]
chunk_max_tokens = 255

[memories.qtest]
store = { qdrant = { url = "http://localhost:6334", collection = "test" } }
dimensions = 3200
embedding_model = "orcamini3b"

[tasks.pythia]
model = "pythia160m"

[tasks.assistant]
model = "mpt_chat"	# The model to use (must be specified above)
prelude = "" # Prompt that is fed once per session to the model
prefix = "<|im_start|>user\n" # Prompt that is fed before each user input (may be multiple in a chat)
postfix = "<|im_end|><|im_start|>assistant\n" # answer<|im_end|> # Prompt that is appended to each user input
private_tokens = ["<|im_start|>", "<|im_end|>"] # Tokens that should never be returned to the user nor accepted in input
stop_sequences = ["<|im_end|>", " stop"] # Text sequences that cause generation to stop (in addition to the end of text token)

[tasks.true_or_false]
model = "mpt_chat"
prelude = "<|im_start|>system\nYou are given statements and determine whether it is true or false.<|im_end|>\n"
prefix = "<|im_start|>user\nIs the following statement true or false: "
postfix = "<|im_end|><|im_start|>assistant\n" # answer<|im_end|>

# When configured, the model will be allowed to freely generate tokens (up to max_tokens) after being fed the prompt.
# Then, the bias prompt will be fed, after which *biased* generation will be performed (using the schema specified).
# Only the tokens generated during *biased* generation are returned. This helps the model 'reason' before output the
# answer in a certain format.
bias_prompt = "<|im_start|>system\nSay 'true' when the user statement was true, 'false' otherwise.<|im_start|>assistant\n"
private_tokens = ["<|im_start|>", "<|im_end|>"]

# JSON schema for the answer. Possible values are (attributes suffixed with '?' are not required):
# { type = "number", min? = 0, max? = 1000, max_decimals? = 2 }
# { type = "array", items? = <any allowed schema defining the schema for items in the array>, min_items? = 1, max_items? = 10 }
# { type = "boolean" }
# { type = "null" }
# { type = "object" } (currently produces an empty object always)
# { type = "string", max_length? = 12, enum? = ["foo", "bar", "baz"] }
biaser = { json_schema = { type = "boolean" } }
temperature = 1

[tasks.cars]
model = "vicuna13b"

# JSON schemas can also be loaded from a file
biaser = { json_schema_file = "./data/cars.schema.json" }